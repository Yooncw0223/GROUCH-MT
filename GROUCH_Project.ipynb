{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mVDRZSsUgoc"
      },
      "source": [
        "# Base Architecture Below\n",
        "\n",
        "# English-to-Spanish translation with a sequence-to-sequence Transformer\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2021/05/26<br>\n",
        "**Last modified:** 2021/05/26<br>\n",
        "**Description:** Implementing a sequence-to-sequene Transformer and training it on a machine translation task.\n",
        "\n",
        "## We modify the above base model by applying our dataset, augmentation methods, and training procedure.\n",
        "### The filename indicates what changes are being measured/experimented here. Note that not all files are here as some files are still being recovered.\n",
        "\n",
        "**Authors:** Chaitanya Ravuri, Karissa Sanchez, Chanwoo Yoon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPM0wpC2Ugod"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we'll build a sequence-to-sequence Transformer model, which\n",
        "we'll train on an English-to-Spanish machine translation task.\n",
        "\n",
        "You'll learn how to:\n",
        "\n",
        "- Vectorize text using the Keras `TextVectorization` layer.\n",
        "- Implement a `TransformerEncoder` layer, a `TransformerDecoder` layer,\n",
        "and a `PositionalEmbedding` layer.\n",
        "- Prepare data for training a sequence-to-sequence model.\n",
        "- Use the trained model to generate translations of never-seen-before\n",
        "input sentences (sequence-to-sequence inference).\n",
        "\n",
        "The code featured here is adapted from the book\n",
        "[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n",
        "(chapter 11: Deep learning for text).\n",
        "The present example is fairly barebones, so for detailed explanations of\n",
        "how each building block works, as well as the theory behind Transformers,\n",
        "I recommend reading the book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT3ToIGnUgoe"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l67QrM5Mlb9",
        "outputId": "9f4c6c9b-c4c2-45fb-b6f2-3780b81681f8"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.9.12 ('tf')' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n tf ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!conda install -n tf ipykernel --update-deps --force-reinstall\n",
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgOzj8haUgoe"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtXnvuLyUgof"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "We'll be working with an English-to-Spanish translation dataset\n",
        "provided by [Anki](https://www.manythings.org/anki/). Let's download it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBrQTb_HUgof",
        "outputId": "705ac7a8-7583-49ef-e4bb-f8eac3c5508d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "text_file = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0UdiFXjUgof"
      },
      "source": [
        "## Parsing the data\n",
        "\n",
        "Each line contains an English sentence and its corresponding Spanish sentence.\n",
        "The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n",
        "We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbYNynBoUgof"
      },
      "outputs": [],
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.split(\"\\t\")\n",
        "    eng = \"[start] \" + eng + \" [end]\"\n",
        "    text_pairs.append((spa, eng))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65HgEzaQUgog"
      },
      "source": [
        "Here's what our sentence pairs look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyGPh4uYUgog",
        "outputId": "7ee2b98c-e3b8-4972-84a3-049bcd66860f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Deberías estar comiendo comida más saludable.', '[start] You should be eating healthier food. [end]')\n",
            "('Comimos fruta fresca después de cenar.', '[start] We ate fresh fruit after dinner. [end]')\n",
            "('Ella quiere que la ayude.', '[start] She wants me to help her. [end]')\n",
            "('Te gusta el inglés, ¿no es cierto?', \"[start] You like English, don't you? [end]\")\n",
            "('Tuvimos unas vacaciones maravillosas.', '[start] We had a wonderful holiday. [end]')\n"
          ]
        }
      ],
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKYFghK4Ugog"
      },
      "source": [
        "Now, let's split the sentence pairs into a training set, a validation set,\n",
        "and a test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WupnvX_mUgog",
        "outputId": "1f68690c-167a-4b57-c3d4-1f9603bc4f61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "118964 total pairs\n",
            "83276 training pairs\n",
            "17844 validation pairs\n",
            "17844 test pairs\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsnLY8CZUgog"
      },
      "source": [
        "## Vectorizing the text data\n",
        "\n",
        "We'll use two instances of the `TextVectorization` layer to vectorize the text\n",
        "data (one for English and one for Spanish),\n",
        "that is to say, to turn the original strings into integer sequences\n",
        "where each integer represents the index of a word in a vocabulary.\n",
        "\n",
        "The English layer will use the default string standardization (strip punctuation characters)\n",
        "and splitting scheme (split on whitespace), while\n",
        "the Spanish layer will use a custom standardization, where we add the character\n",
        "`\"¿\"` to the set of punctuation characters to be stripped.\n",
        "\n",
        "Note: in a production-grade machine translation model, I would not recommend\n",
        "stripping the punctuation characters in either language. Instead, I would recommend turning\n",
        "each punctuation character into its own token,\n",
        "which you could achieve by providing a custom `split` function to the `TextVectorization` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLnqVOuHUgoh"
      },
      "outputs": [],
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
        ")\n",
        "spa_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcrQsqarUgoh"
      },
      "source": [
        "Next, we'll format our datasets.\n",
        "\n",
        "At each training step, the model will seek to predict target words N+1 (and beyond)\n",
        "using the source sentence and the target words 0 to N.\n",
        "\n",
        "As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n",
        "\n",
        "- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n",
        "`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n",
        "that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n",
        "- `target` is the target sentence offset by one step:\n",
        "it provides the next words in the target sentence -- what the model will try to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmyKE1_VUgoh"
      },
      "outputs": [],
      "source": [
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = spa_vectorization(spa)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UXZ40n8Ugoh"
      },
      "source": [
        "Let's take a quick look at the sequence shapes\n",
        "(we have batches of 64 pairs, and all sequences are 20 steps long):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfb4IHNUUgoh",
        "outputId": "be53eebf-c771-4d1a-83cf-711eb1f637d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al5PBF-8e9kZ"
      },
      "source": [
        "## Making the low-resource datasets\n",
        "\n",
        "For our low resource datasets, we take 10% of the high resource dataset. The dataset is already shuffled, so we can just take the first 10%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQXt_OAefAVX",
        "outputId": "9bb3cfee-a215-4e56-eae5-15c2165a95b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8327 low resource training pairs\n",
            "1784 low resource validation pairs\n"
          ]
        }
      ],
      "source": [
        "frac_low_resource = 0.1\n",
        "\n",
        "train_pairs_low = train_pairs[:int(frac_low_resource * len(train_pairs))]\n",
        "val_pairs_low = val_pairs[:int(frac_low_resource * len(val_pairs))]\n",
        "\n",
        "train_eng_texts_low = [pair[0] for pair in train_pairs_low]\n",
        "train_spa_texts_low = [pair[1] for pair in train_pairs_low]\n",
        "eng_vectorization.adapt(train_eng_texts_low)\n",
        "spa_vectorization.adapt(train_spa_texts_low)\n",
        "\n",
        "train_ds_low = make_dataset(train_pairs_low)\n",
        "val_ds_low = make_dataset(val_pairs_low)\n",
        "\n",
        "print(f\"{len(train_pairs_low)} low resource training pairs\")\n",
        "print(f\"{len(val_pairs_low)} low resource validation pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF1hdGcpUgoh"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n",
        "and a `TransformerDecoder` chained together. To make the model aware of word order,\n",
        "we also use a `PositionalEmbedding` layer.\n",
        "\n",
        "The source sequence will be pass to the `TransformerEncoder`,\n",
        "which will produce a new representation of it.\n",
        "This new representation will then be passed\n",
        "to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n",
        "The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n",
        "\n",
        "A key detail that makes this possible is causal masking\n",
        "(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n",
        "The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n",
        "sure that it only uses information from target tokens 0 to N when predicting token N+1\n",
        "(otherwise, it could use information from the future, which would\n",
        "result in a model that cannot be used at inference time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-b99S6xUgoi"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"latent_dim\": self.latent_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rouRHcGUgoi"
      },
      "source": [
        "Next, we assemble the end-to-end model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56n-ni0UUgoi"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQiIhF-ZUgoi"
      },
      "source": [
        "## Training our model\n",
        "\n",
        "We'll use accuracy as a quick way to monitor training progress on the validation data.\n",
        "Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n",
        "\n",
        "Here we only train for 1 epoch, but to get the model to actually converge\n",
        "you should train for at least 30 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7QpTamgUgoi",
        "outputId": "7b2e2120-6f62-4e8d-d81f-4540e68b18c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " positional_embedding_2 (Positi  (None, None, 256)   3845120     ['encoder_inputs[0][0]']         \n",
            " onalEmbedding)                                                                                   \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer_encoder_1 (Transfo  (None, None, 256)   3155456     ['positional_embedding_2[0][0]'] \n",
            " rmerEncoder)                                                                                     \n",
            "                                                                                                  \n",
            " model_3 (Functional)           (None, None, 15000)  12959640    ['decoder_inputs[0][0]',         \n",
            "                                                                  'transformer_encoder_1[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 19,960,216\n",
            "Trainable params: 19,960,216\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/30\n",
            "131/131 [==============================] - 13s 75ms/step - loss: 2.1206 - accuracy: 0.2858 - val_loss: 1.8066 - val_accuracy: 0.3559\n",
            "Epoch 2/30\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 1.6648 - accuracy: 0.3976 - val_loss: 1.6702 - val_accuracy: 0.4183\n",
            "Epoch 3/30\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 1.4863 - accuracy: 0.4560 - val_loss: 1.4974 - val_accuracy: 0.4592\n",
            "Epoch 4/30\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 1.3336 - accuracy: 0.5036 - val_loss: 1.4540 - val_accuracy: 0.4863\n",
            "Epoch 5/30\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 1.2154 - accuracy: 0.5443 - val_loss: 1.4104 - val_accuracy: 0.4972\n",
            "Epoch 6/30\n",
            "131/131 [==============================] - 10s 74ms/step - loss: 1.0969 - accuracy: 0.5833 - val_loss: 1.3491 - val_accuracy: 0.5188\n",
            "Epoch 7/30\n",
            "131/131 [==============================] - 9s 71ms/step - loss: 0.9985 - accuracy: 0.6180 - val_loss: 1.4188 - val_accuracy: 0.5232\n",
            "Epoch 8/30\n",
            "131/131 [==============================] - 9s 71ms/step - loss: 0.9128 - accuracy: 0.6478 - val_loss: 1.3307 - val_accuracy: 0.5382\n",
            "Epoch 9/30\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 0.8300 - accuracy: 0.6765 - val_loss: 1.3453 - val_accuracy: 0.5415\n",
            "Epoch 10/30\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 0.7540 - accuracy: 0.7025 - val_loss: 1.3241 - val_accuracy: 0.5418\n",
            "Epoch 11/30\n",
            "131/131 [==============================] - 9s 69ms/step - loss: 0.6967 - accuracy: 0.7256 - val_loss: 1.3317 - val_accuracy: 0.5510\n",
            "Epoch 12/30\n",
            "131/131 [==============================] - 9s 69ms/step - loss: 0.6353 - accuracy: 0.7473 - val_loss: 1.3500 - val_accuracy: 0.5490\n",
            "Epoch 13/30\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 0.5872 - accuracy: 0.7685 - val_loss: 1.3375 - val_accuracy: 0.5568\n",
            "Epoch 14/30\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 0.5464 - accuracy: 0.7832 - val_loss: 1.4054 - val_accuracy: 0.5518\n",
            "Epoch 15/30\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 0.5121 - accuracy: 0.7976 - val_loss: 1.3679 - val_accuracy: 0.5590\n",
            "Epoch 16/30\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 0.4867 - accuracy: 0.8067 - val_loss: 1.3650 - val_accuracy: 0.5584\n",
            "Epoch 17/30\n",
            "131/131 [==============================] - 9s 69ms/step - loss: 0.4644 - accuracy: 0.8179 - val_loss: 1.3725 - val_accuracy: 0.5636\n",
            "Epoch 18/30\n",
            "131/131 [==============================] - 9s 69ms/step - loss: 0.4461 - accuracy: 0.8257 - val_loss: 1.3863 - val_accuracy: 0.5554\n",
            "Epoch 19/30\n",
            "131/131 [==============================] - 9s 69ms/step - loss: 0.4308 - accuracy: 0.8350 - val_loss: 1.3862 - val_accuracy: 0.5574\n",
            "Epoch 20/30\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 0.4209 - accuracy: 0.8399 - val_loss: 1.4020 - val_accuracy: 0.5572\n",
            "Epoch 21/30\n",
            "131/131 [==============================] - 9s 69ms/step - loss: 0.4147 - accuracy: 0.8437 - val_loss: 1.4228 - val_accuracy: 0.5576\n",
            "Epoch 22/30\n",
            "131/131 [==============================] - 9s 69ms/step - loss: 0.4062 - accuracy: 0.8493 - val_loss: 1.4481 - val_accuracy: 0.5577\n",
            "Epoch 23/30\n",
            "131/131 [==============================] - 9s 69ms/step - loss: 0.4007 - accuracy: 0.8537 - val_loss: 1.4553 - val_accuracy: 0.5589\n",
            "Epoch 24/30\n",
            "131/131 [==============================] - 9s 69ms/step - loss: 0.3940 - accuracy: 0.8568 - val_loss: 1.4394 - val_accuracy: 0.5639\n",
            "Epoch 25/30\n",
            "131/131 [==============================] - 9s 69ms/step - loss: 0.3913 - accuracy: 0.8614 - val_loss: 1.4395 - val_accuracy: 0.5568\n",
            "Epoch 26/30\n",
            "131/131 [==============================] - 9s 72ms/step - loss: 0.3840 - accuracy: 0.8655 - val_loss: 1.5161 - val_accuracy: 0.5514\n",
            "Epoch 27/30\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 0.3790 - accuracy: 0.8674 - val_loss: 1.4383 - val_accuracy: 0.5652\n",
            "Epoch 28/30\n",
            "131/131 [==============================] - 9s 69ms/step - loss: 0.3765 - accuracy: 0.8697 - val_loss: 1.4511 - val_accuracy: 0.5653\n",
            "Epoch 29/30\n",
            "131/131 [==============================] - 9s 70ms/step - loss: 0.3728 - accuracy: 0.8714 - val_loss: 1.4554 - val_accuracy: 0.5615\n",
            "Epoch 30/30\n",
            "131/131 [==============================] - 9s 69ms/step - loss: 0.3708 - accuracy: 0.8739 - val_loss: 1.4566 - val_accuracy: 0.5665\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f51fb5fe7c0>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epochs = 30  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "transformer.fit(train_ds_low, epochs=epochs, validation_data=val_ds_low)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmEKkcXdUgoi"
      },
      "source": [
        "## Decoding test sentences\n",
        "\n",
        "Finally, let's demonstrate how to translate brand new English sentences.\n",
        "We simply feed into the model the vectorized English sentence\n",
        "as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n",
        "we hit the token `\"[end]\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3MPyo6-Ugoi",
        "outputId": "d68f358e-3446-4102-ef15-2da7d84b658a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probablemente él terminará el trabajo para mañana. \t [start] probably probably it for the job tomorrow [end]\n",
            "Tom lavó mucha ropa hoy. \t [start] tom did very do today [end]\n",
            "Ella es una de las mejores bailarinas del mundo. \t [start] she is a left of the best of the world [end]\n",
            "Estuve pensando en ti el día de hoy. \t [start] i was at you thinking of me today [end]\n",
            "Mi padre se dedica al comercio exterior. \t [start] my father became him to the speak [end]\n",
            "Detesto ser una molestia. \t [start] i hate to be a mary [end]\n",
            "Tomás quiere ser útil. \t [start] tom wants to be try [end]\n",
            "Los detalles del plan fueron revelados. \t [start] details will be out of the plan [end]\n",
            "Dile a Tom que voy. \t [start] tell tom [end]\n",
            "En este momento están descansando. \t [start] in this time is it [end]\n",
            "Guárdalo. \t [start] keep it [end]\n",
            "Ustedes no deberían haberlo dicho. \t [start] you shouldnt have too much you [end]\n",
            "De vez en cuando ella juega tenis. \t [start] lets play in when she is tennis [end]\n",
            "Está la posibilidad de que Tom llegue tarde. \t [start] is the names of them tom [end]\n",
            "Tiene tres hermanas mayores. \t [start] there has lost my favorite [end]\n",
            "Todo pasó muy rápido. \t [start] everything happened very fast [end]\n",
            "Comprá un libro y leelo. \t [start] keep a book and up [end]\n",
            "Tom le explicó el proyecto a Mary. \t [start] tom is explained the those to mary [end]\n",
            "La genialidad es uno por ciento inspiración y noventa y nueve por ciento transpiración. \t [start] first of paper is he are married and one who [end]\n",
            "Lágrimas cayeron de sus ojos. \t [start] eyes on the eyes [end]\n",
            "El anciano bajó del autobús. \t [start] the old man out of the bus [end]\n",
            "Puedo volver a hacerlo. \t [start] i can go now [end]\n",
            "Voy de regreso a mi oficina. \t [start] im going to the went to the office [end]\n",
            "Este artículo ridiculiza a los vegetarianos. \t [start] this need to sing i mary [end]\n",
            "Agotado del día de trabajo, se fue a la cama mucho más temprano de lo habitual. \t [start] stop the day [end]\n",
            "No tengo idea por qué Tom está tan nervioso. \t [start] i have no idea why tom is so [end]\n",
            "Preguntémosle al profesor. \t [start] his the teacher [end]\n",
            "Lo tengo a él para que pinte la cerca. \t [start] i have it to had to what him to the last time [end]\n",
            "Tom todavía no tiene suficiente dinero como para comprar el coche que él quiere. \t [start] tom still didnt have enough money as to buy the car as if he has [end]\n",
            "Creo que deberías esperar. \t [start] i think you should wait [end]\n"
          ]
        }
      ],
      "source": [
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def lookup_words(x, vocab):\n",
        "  return [vocab[i] for i in x]\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    output = []\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(input_sentence, '\\t', translated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6QDqzQiUgoi"
      },
      "source": [
        "After 30 epochs, we get results such as:\n",
        "\n",
        "> She handed him the money.\n",
        "> [start] ella le pasó el dinero [end]\n",
        "\n",
        "> Tom has never heard Mary sing.\n",
        "> [start] tom nunca ha oído cantar a mary [end]\n",
        "\n",
        "> Perhaps she will come tomorrow.\n",
        "> [start] tal vez ella vendrá mañana [end]\n",
        "\n",
        "> I love to write.\n",
        "> [start] me encanta escribir [end]\n",
        "\n",
        "> His French is improving little by little.\n",
        "> [start] su francés va a [UNK] sólo un poco [end]\n",
        "\n",
        "> My hotel told me to call you.\n",
        "> [start] mi hotel me dijo que te [UNK] [end]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq-6IpW_MfqF"
      },
      "outputs": [],
      "source": [
        "import sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_yF-yE8Mb5i"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import bleu\n",
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    output = [spa_vectorization([\"[start]\"])[0, 0]]\n",
        "\n",
        "    # output = spa_vectorization([\"[start]\"])[:, :-1]\n",
        "    print(output)\n",
        "\n",
        "    # decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        # tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, output])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        output[i+1] = sampled_token_index\n",
        "        # print(sampled_token_index)\n",
        "\n",
        "        # sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        # decoded_sentence += \" \" + sampled_token\n",
        "        # print(spa_vectorization([decoded_sentence])[:, :-1])\n",
        "        # break\n",
        "\n",
        "        if spa_index_lookup[sampled_token_index] == \"[end]\":\n",
        "            break\n",
        "    print(output)\n",
        "    return output\n",
        "\n",
        "def compute_BLEU(test_pairs):\n",
        "  bleu_scores = []\n",
        "  for i, (x, y) in enumerate(test_pairs):\n",
        "    out = decode_sequence(x)\n",
        "    # print(out, '|', y)\n",
        "    bleu_scores.append(sacrebleu.raw_corpus_bleu([out], [[y]], .01).score)\n",
        "    break\n",
        "    # if i % 100 == 0:\n",
        "    #   print(i, np.mean(bleu_scores))\n",
        "\n",
        "  return np.mean(bleu_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "bOgXysncNctp",
        "outputId": "7ad4f6fb-3181-4ac7-b42f-a1849137fa42"
      },
      "outputs": [],
      "source": [
        "print(compute_BLEU(test_pairs[:100]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlFp1P4FOH0Z"
      },
      "outputs": [],
      "source": [
        "transformer.save_weights('./checkpoints/checkpoint1')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "8370698ee37a82dba411816cad6d30ae4ddad961dc32b2555c26c5e8dfc955bf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
